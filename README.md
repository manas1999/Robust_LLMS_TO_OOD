### README for Fine-Tuning and Testing Language Models

This repository contains scripts for fine-tuning and testing large language models with options for full fine-tuning, LoRA, and QLoRA adaptation methods. It supports generating synthetic data for training and provides functionality for testing the fine-tuned models with custom prompts.

#### Requirements

- Python 3.6 or later
- `transformers`
- `torch`
- `numpy`
- `peft` (for LoRA and QLoRA adaptation)
- `datasets`

#### Setup

1. **Install Dependencies**: Ensure all required libraries are installed. You can install them using pip:

```bash
pip install transformers torch numpy datasets
pip install git+https://github.com/path/to/peft.git # Assuming peft is not available on PyPI and needs to be installed from a Git repository
```

2. **API Token**: To access pre-trained models from Hugging Face, you may need an API token. Set this token in the script or use the `use_auth_token` parameter when loading models and tokenizers.

#### Usage

The main script is designed to be used from the command line with several options for customization:

- `--model_name`: The name of the model you want to fine-tune or test.
- `--finetune`: Specify the fine-tuning method (`lora`, `qlora`, or `full_finetune`).
- `--infer`: Enable this flag to test the model directly without fine-tuning.
- `--test_data`: Path to the test data file (default: "test_dataset.txt").
- `--prompt`: Prompt string for testing the model in inference mode.
- `--data`: Choose the data source for fine-tuning: `synthetic` data or an external `dataset`.

**Example Command**:

```bash
python main.py --model_name="chargoddard/Yi-34B-Llama" --finetune="lora" --data="synthetic"
```

This command will fine-tune the `chargoddard/Yi-34B-Llama` model using the LoRA method on synthetic data generated by the script.

#### Fine-Tuning Methods

- **Full Fine-Tuning**: Updates all parameters of the model.
- **LoRA**: Low-Rank Adaptation focuses on updating only a small set of parameters, reducing the computational cost.
- **QLoRA**: A quantized version of LoRA that further reduces the model size and computational requirements.

#### Data Generation

The script can generate synthetic data if the `--data` option is set to `synthetic`. This data is used for fine-tuning when no external dataset is provided.

#### Testing and Inference

The model can be tested with out-of-domain data or used for inference with custom prompts. Provide a test dataset or a prompt as per your requirements.

#### Notes

- Make sure the paths and model names are correctly specified.
- The performance of fine-tuned models may vary based on the data and parameters used.
- LoRA and QLoRA require specific configurations and understanding of the model's architecture for optimal results.

#### Contributions

Contributions to improve the scripts or extend functionality are welcome. Please follow the standard GitHub pull request process.

#### License

Specify your license here.